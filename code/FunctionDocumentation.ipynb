{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Import CountVectorizer and TFIDFVectorizer from feature_extraction.text.\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source material attributed to General Assembly course notes and online documentation centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorizer Section\n",
    "\n",
    "## Pre-Processing\n",
    "\n",
    "Let's review some of the pre-processing steps for text data:\n",
    "\n",
    "- Remove special characters\n",
    "- Tokenizing\n",
    "- Lemmatizing/Stemming\n",
    "- Stop word removal\n",
    "\n",
    "`CountVectorizer` actually can do a lot of this for us! It is important to keep these steps in mind in case you want to change the default methods used for each of these. Full documentation may be found here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `CountVectorizer`\n",
    "---\n",
    "\n",
    "The easiest way for us to convert text data into a structured, numeric `X` dataframe is to use `CountVectorizer`.\n",
    "\n",
    "- **Count**: Count up how many times a token is observed in a given document.\n",
    "- **Vectorizer**: Create a column (also known as a vector) that stores those counts.\n",
    "\n",
    "![Count Vectorizer In Action](../images_FunctionDocumentation/countvectorizer2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a CountVectorizer.\n",
    "cvec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation Step using the Count Vectorizer on training and testing data chosen features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Count Vectorizer during the Tranformation](../images_FunctionDocumentation/countvectorizer.png)\n",
    "\n",
    "[Source](https://towardsdatascience.com/nlp-learning-series-part-2-conventional-methods-for-text-classification-40f2839dd061)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Step of chosen function along with Count Vectorizer. \n",
    "\n",
    "#### Please Note: It is also required for optimize parameters for both `Count Vectorizer` as well as the desired modeling function of choice! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, verify accuracy scores and other classification metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## These are the steps that MUST be followed for a general Natural Langauge Processing Model to be successful. Next, the Tfidf Vectorizer is introduced. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before moving on, it is important to express that one must utilize GridSearchCV with the desired function to obtain optimal parameters for the function given the data inputs. The documentation for this function may be found here: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `CountVectorizer` has been explored in how it may transform text data into something passable through a model.\n",
    "\n",
    "But what if something more than just counting the occurrences of each token is required?\n",
    "\n",
    "# Term Frequency-Inverse Document Frequency (TF-IDF) Vectorizer Section\n",
    "\n",
    "---\n",
    "\n",
    "When modeling, which words tend to be the most helpful?\n",
    "- Words that are common across all documents.\n",
    "- Words that are rare across all documents.\n",
    "- Words that are rare across some documents, and common across some documents.\n",
    "\n",
    "The answer to this question is that words that are common in certain documents but rare in other documents tend to be more informative than words that are common in all documents or rare in all documents. For example: If one were to examine poetry over time, the word \"thine\" might be common in some documents but rare in most documents. The word \"thine\" is probably pretty informative in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF is a score that tells us which words are important to one document, relative to all other documents. Words that occur often in one document but don't occur in many documents contain more predictive power. Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "***Breaking it down:***\n",
    "\n",
    "*Term Frequency* is the number of times a word appears in the document (same as values in CountVectorizer).\n",
    "\n",
    "*Document Frequency* is the percentage of documents that a particular word appears in. \n",
    "\n",
    "$$ \\log \\frac{1+n}{1+df(t)}  + 1 $$\n",
    "\n",
    "> where $n$ is the total number of documents in the document set, and *df(t)* is the number of documents in the document set that contain term *t*. The resulting tf-idf vectors are then normalized by the Euclidean norm... - the [docs](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)\n",
    "\n",
    "Variations of the TF-IDF score are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.\n",
    "\n",
    "![Tfidf Vectorizer In Action](../images_FunctionDocumentation/tfidfvectorizer.png)\n",
    "\n",
    "[Source](https://towardsdatascience.com/nlp-learning-series-part-2-conventional-methods-for-text-classification-40f2839dd061)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From there, the process is the same as `Count Vectorizer`, there is a transformation, a fit, and an assessment of model performance. \n",
    "\n",
    "#### Please Note: It is also required for optimize parameters for both `Count Vectorizer` as well as `TF-IDF Vectorizer`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Documentation: \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine Documentation: \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE Documentation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source material attributed to General Assembly course notes and online documentation centers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
